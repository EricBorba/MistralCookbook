{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLq3hMOP1KR"
   },
   "source": [
    "# Evaluating RAG: Using Mistral Models for LLM as a Judge (With Structured Outputs)\n",
    "\n",
    "This cookbook shows an example of using the Mistral AI models for LLM As A Judge using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZhlfwDMP5hj"
   },
   "source": [
    "## Imports & API Key Setting\n",
    "You can get your api key from: https://console.mistral.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vB4dHYDU9mST",
    "outputId": "e6ef92d6-778f-4ee5-9783-5ee1d6078a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->mistralai) (4.8.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->mistralai) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Mistral AI API Key ········\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from getpass import getpass\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Define the API key and model\n",
    "api_key = getpass(\"Enter Mistral AI API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfYrV_vOQIwY"
   },
   "source": [
    "## Main Code For LLM As A Judge For RAG (With Structured Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXc7OkcP47dF",
    "outputId": "4168cde7-bc7a-479e-edd2-13174281e538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 RAG Evaluation:\n",
      "\n",
      "Criteria: Context Relevance\n",
      "Reasoning: The retrieved context provides a basic definition of renewable energy and lists some examples, but it does not directly address the benefits of renewable energy, which is what the query asks for. Therefore, it is somewhat relevant but not fully aligned with the query's intent.\n",
      "Score: 1/3\n",
      "\n",
      "Criteria: Answer Relevance\n",
      "Reasoning: The generated answer addresses the user's query about the benefits of renewable energy. It highlights that renewable energy sources like solar and wind are environmentally friendly and reduce carbon emissions, which are significant benefits. However, it does not mention other potential benefits such as energy independence or long-term cost savings.\n",
      "Score: 2/3\n",
      "\n",
      "Criteria: Groundedness\n",
      "Reasoning: The generated answer is somewhat grounded in the retrieved context as it mentions solar and wind energy, which are examples of renewable energy listed in the context. However, the benefits mentioned in the answer (environmentally friendly and reducing carbon emissions) are not explicitly stated in the context.\n",
      "Score: 1/3\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from getpass import getpass\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "client = Mistral(api_key=api_key)\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "# Define Enum for scores\n",
    "class Score(str, Enum):\n",
    "    no_relevance = \"0\"\n",
    "    low_relevance = \"1\"\n",
    "    medium_relevance = \"2\"\n",
    "    high_relevance = \"3\"\n",
    "\n",
    "# Define a constant for the score description\n",
    "SCORE_DESCRIPTION = (\n",
    "    \"Score as a string between '0' and '3'. \"\n",
    "    \"0: No relevance/Not grounded/Irrelevant - The context/answer is completely unrelated or not based on the context. \"\n",
    "    \"1: Low relevance/Low groundedness/Somewhat relevant - The context/answer has minimal relevance or grounding. \"\n",
    "    \"2: Medium relevance/Medium groundedness/Mostly relevant - The context/answer is somewhat relevant or grounded. \"\n",
    "    \"3: High relevance/High groundedness/Fully relevant - The context/answer is highly relevant or grounded.\"\n",
    ")\n",
    "\n",
    "# Define separate classes for each criterion with detailed descriptions\n",
    "class ContextRelevance(BaseModel):\n",
    "    explanation: str = Field(..., description=(\"Step-by-step reasoning explaining how the retrieved context aligns with the user's query. \"\n",
    "                    \"Consider the relevance of the information to the query's intent and the appropriateness of the context \"\n",
    "                    \"in providing a coherent and useful response.\"))\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "class AnswerRelevance(BaseModel):\n",
    "    explanation: str = Field(..., description=(\"Step-by-step reasoning explaining how well the generated answer addresses the user's original query. \"\n",
    "                    \"Consider the helpfulness and on-point nature of the answer, aligning with the user's intent and providing valuable insights.\"))\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "class Groundedness(BaseModel):\n",
    "    explanation: str = Field(..., description=(\"Step-by-step reasoning explaining how faithful the generated answer is to the retrieved context. \"\n",
    "                    \"Consider the factual accuracy and reliability of the answer, ensuring it is grounded in the retrieved information.\"))\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "class RAGEvaluation(BaseModel):\n",
    "    context_relevance: ContextRelevance = Field(..., description=\"Evaluation of the context relevance to the query, considering how well the retrieved context aligns with the user's intent.\" )\n",
    "    answer_relevance: AnswerRelevance = Field(..., description=\"Evaluation of the answer relevance to the query, assessing how well the generated answer addresses the user's original query.\" )\n",
    "    groundedness: Groundedness = Field(..., description=\"Evaluation of the groundedness of the generated answer, ensuring it is faithful to the retrieved context.\" )\n",
    "\n",
    "# Function to evaluate RAG metrics\n",
    "def evaluate_rag(query: str, retrieved_context: str, generated_answer: str):\n",
    "    chat_response = client.chat.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a judge for evaluating a Retrieval-Augmented Generation (RAG) system. \"\n",
    "                    \"Evaluate the context relevance, answer relevance, and groundedness based on the following criteria: \"\n",
    "                    \"Provide a reasoning and a score as a string between '0' and '3' for each criterion. \"\n",
    "                    \"Context Relevance: How relevant is the retrieved context to the query? \"\n",
    "                    \"Answer Relevance: How relevant is the generated answer to the query? \"\n",
    "                    \"Groundedness: How faithful is the generated answer to the retrieved context?\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Query: {query}\\nRetrieved Context: {retrieved_context}\\nGenerated Answer: {generated_answer}\"\n",
    "            },\n",
    "        ],\n",
    "        response_format=RAGEvaluation,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_response.choices[0].message.parsed\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the benefits of renewable energy?\"\n",
    "retrieved_context = \"Renewable energy includes solar, wind, hydro, and geothermal energy, which are naturally replenished.\"\n",
    "generated_answer = \"Renewable energy sources like solar and wind are environmentally friendly and reduce carbon emissions.\"\n",
    "evaluation = evaluate_rag(query, retrieved_context, generated_answer)\n",
    "\n",
    "# Print the evaluation\n",
    "print(\"🏆 RAG Evaluation:\")\n",
    "print(\"\\nCriteria: Context Relevance\")\n",
    "print(f\"Reasoning: {evaluation.context_relevance.explanation}\")\n",
    "print(f\"Score: {evaluation.context_relevance.score.value}/3\")\n",
    "\n",
    "print(\"\\nCriteria: Answer Relevance\")\n",
    "print(f\"Reasoning: {evaluation.answer_relevance.explanation}\")\n",
    "print(f\"Score: {evaluation.answer_relevance.score.value}/3\")\n",
    "\n",
    "print(\"\\nCriteria: Groundedness\")\n",
    "print(f\"Reasoning: {evaluation.groundedness.explanation}\")\n",
    "print(f\"Score: {evaluation.groundedness.score.value}/3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_f35mXT9jVF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
